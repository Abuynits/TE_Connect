{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TE_Connect Sales Forcasting\n",
        "\n",
        "Author: Alexiy Buynitsky\n",
        "\n",
        "### Configuration\n",
        "\n",
        "Run Through the following commands, and then alter the:\n",
        "- `src/data_constants.py`\n",
        "  -  Control data inputs,outputs, prediction, foracsting...\n",
        "- `src/model_constants.py`\n",
        "  - Control model architecture, lstm cells, hidden layers, dropout...\n",
        "- `src/filepath_constants.py`\n",
        "  - Control file system, verbose mode...\n",
        "\n",
        "You then only have to run the last command and use the [MLflow UI](https://dagshub.com/Abuynits/TE_Connect.mlflow) to track your experiments!\n",
        "\n"
      ],
      "metadata": {
        "id": "4E5MrKGAuh_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "WTFH7T6av-eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install -r requirements.txt &> /dev/null\n",
        "!pip3 install pytorch_lightning &> /dev/null\n",
        "# install all libraries\n",
        "!pip install dvc &> /dev/null\n",
        "!pip install dagshub &> /dev/null\n",
        "!pip3 install mlflow &> /dev/null"
      ],
      "metadata": {
        "id": "xeyzQVCqo-Be"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "igD1KMfNmD9b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import getpass\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Environment Variables\n",
        "\n",
        "#@markdown Enter the repository name for the project:\n",
        "REPO_NAME= \"TE_Connect\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the username of your DAGsHub account:\n",
        "USER_NAME = \"Abuynits\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the email for your DAGsHub account:\n",
        "EMAIL = \"abuynits@gmail.com\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "id": "Zz8pia1hmRTl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a personal access token\n",
        "r = requests.post('https://dagshub.com/api/v1/user/tokens', \n",
        "                  json={\"name\": f\"colab-token-{datetime.datetime.now()}\"}, \n",
        "                  auth=(USER_NAME, getpass.getpass('DAGsHub password:')))\n",
        "r.raise_for_status()\n",
        "TOKEN=r.json()['sha1']"
      ],
      "metadata": {
        "id": "BjmO8-jqmS_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9da651-c911-4475-9b86-3c991d3b8e3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DAGsHub password:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating SSH Key for github"
      ],
      "metadata": {
        "id": "YTbsb_y7wBoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -b 4096"
      ],
      "metadata": {
        "id": "DPf7E-lTmWfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465457f6-7176-4931-fe98-643bd81e851c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_rsa): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:RBBq6okish2EF0AjlkFy5WMXBs29sFvJcEVruoFGF5k root@07def59b38d8\n",
            "The key's randomart image is:\n",
            "+---[RSA 4096]----+\n",
            "|**+.o+=+o=o      |\n",
            "|++.. o=oE. .     |\n",
            "|  . * o*ooo      |\n",
            "| . = +.+=o       |\n",
            "|. +   ooS        |\n",
            "| = . ..  o       |\n",
            "|= +     .        |\n",
            "|+o .             |\n",
            "|. .              |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hit enter for all to use default file location. \n",
        "\n",
        "**NOTE**: will be deleted at the end of the notebook session"
      ],
      "metadata": {
        "id": "A8lF8jxKwEWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts"
      ],
      "metadata": {
        "id": "GoW-bLGlmhv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12889406-f488-4a2b-a910-81ff45d62b1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "# github.com:22 SSH-2.0-babeld-2ef5a5a9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the ssh_id and add it to github keys. Will only be valid for this notebook session"
      ],
      "metadata": {
        "id": "d_lqAVq9wNyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "id": "IQtYBMZgnXv8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d508a61a-a163-411a-92e1-976c47ed5589"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDjiWIAKUpx0X2ojjK7ApzTVBqmlSDXAbwHSXwwdzyuY15I8voEbH4NDNlqdCqGOpw4pOYXv4pi2F3sN55tOoTTijeO3Gc//8pvIrTM3rzF4uYYfiNtoFZsHiRwBeT4lagVcabOCJFh9iuI1EUi6HXwsVWSir/jEyvS4tm1GdkNNJu5FzbKSCXB+Q2pghBXeNgrmtPkXX+sq4noypNyZqtYT+u94tZA1LoP9rPyPUmDnn1RQlNQfCFzS+zgpCIeBcMrr6GCV64nfYS701p8bBAr03wPDd3K0E1/SePce6W9v/481F+0O3u/01ytAzD0pHb+x5a//CVnZfKaNjwhlnmJAZCfupjjuRS9iZoKt3DBHz3xUl0gL22Nv+JxsyzSOyTligSq/ohThNXvtgWATcSsLabbO4KHRJ9CbzIqTzCKVo1G5d7eb7a9q+otyQDIYxu6Olt3Rhjoyklaiqh/bLHanlWNWrK6dy4Ou499zrDlIGRwjMOlW8lGdn0zh0bRa+S9sTZg8ofOkWDKgO+ycJDPxZH0+o94sAKzH6iS9Qzq7jJaCv1q+ioTDOCqkxRsunAhgoB42FOWoufL4JGu5mOpLS1ed8aQ7wxBF9mfzf1FiWLPYhJ7EI9StTprCnMM/g7T+AiLYiLIc02EQeGzdmqJWxwktBcVsQQYKJT6R5YlXw== root@07def59b38d8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh -T git@github.com # test the ssh id  "
      ],
      "metadata": {
        "id": "lFZRBI17naoi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb2322c-5e6b-4775-9ce9-a4c5310b4145"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added the RSA host key for IP address '20.205.243.166' to the list of known hosts.\r\n",
            "Hi Abuynits! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configure github usernames\n",
        "!git config --global user.email {REPO_NAME} \n",
        "!git config --global user.name {USER_NAME}"
      ],
      "metadata": {
        "id": "891nyLWGnsn0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone git@github.com:Abuynits/TE_Connect.git"
      ],
      "metadata": {
        "id": "E4OhvxDLn0oc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd83da0-912e-43d8-aa89-5b4b8f558cc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TE_Connect'...\n",
            "remote: Enumerating objects: 660, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 660 (delta 110), reused 97 (delta 43), pack-reused 491\u001b[K\n",
            "Receiving objects: 100% (660/660), 63.54 MiB | 7.83 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TE_Connect"
      ],
      "metadata": {
        "id": "z6UnbvARoTRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b61824-c8de-4606-aa14-1970d4f788ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TE_Connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize DVC!"
      ],
      "metadata": {
        "id": "a_RGEKBDwYH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc remote modify origin --local auth basic \n",
        "!dvc remote modify origin --local user {USER_NAME} \n",
        "!dvc remote modify origin --local password {TOKEN}"
      ],
      "metadata": {
        "id": "ryTLZ3wnoGqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68b09256-9be6-4e15-8ea5-1c9cdde6c969"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[0m\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc pull --remote origin"
      ],
      "metadata": {
        "id": "2V_pj4SnoJw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0fa6d5-10d6-453b-dfcb-91b3cf06b42f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING\u001b[39m: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\n",
            "name: data/dicts/validation_dict.pkl, md5: a4f898c0c6449dc10167c14c77e09d89\n",
            "name: data/model_io/test_y.pkl, md5: d25cac3447bfdead313dea1377ceb717\n",
            "name: data/model_io/validation_x.pkl, md5: f98d83ac13933adac7eb68a2156d56e6\n",
            "name: data/model_io/validation_y.pkl, md5: c2e645e1cbd1441ddecc1a20859850e1\n",
            "name: data/model_io/train_x.pkl, md5: df34654782d9068b7a7161247be93f6d\n",
            "name: data/dicts/train_dict.pkl, md5: 27011e6457f33460d8d3e90b14aa815c\n",
            "name: data/model_io/test_x.pkl, md5: b6c0e7709bba19e0985423b586a9a080\n",
            "name: data/dicts/test_dict.pkl, md5: 509e355771ac85cc42a1445fb07f6876\n",
            "name: data/model_io/train_y.pkl, md5: c5c17ba4762d3460fe794f03a4dd4bda\n",
            "Transferring:   0% 0/6 [00:00<?, ?file/s{'info': ''}]  \n",
            "!\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T89.2k/? [00:00<00:00,     752kB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE160k/? [00:00<00:00,     727kB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE190k/? [00:00<00:00,     495kB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE291k/? [00:00<00:00,     654kB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE458k/? [00:00<00:00,     955kB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE670k/? [00:00<00:00,    1.31MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE916k/? [00:00<00:00,    1.66MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.26M/? [00:00<00:00,    2.32MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.76M/? [00:00<00:00,    3.15MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T2.34M/? [00:01<00:00,    4.01MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T3.15M/? [00:01<00:00,    5.32MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T4.39M/? [00:01<00:00,    7.57MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T5.83M/? [00:01<00:00,    9.81MB/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T7.91M/? [00:01<00:00,    13.4MB/s]\u001b[A\n",
            "Transferring:  17% 1/6 [00:02<00:10,  2.07s/file{'info': ''}]\n",
            "                                                                                \u001b[A\n",
            "!\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "\n",
            "!\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "!\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Transferring:  33% 2/6 [00:02<00:04,  1.08s/file{'info': ''}]\n",
            "\n",
            "                                                                                \u001b[A\u001b[A\n",
            "\n",
            "!\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE216k/? [00:00<00:00,    2.19MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "!\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE_Con0.00/? [00:00<?,        ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T69.7k/? [00:00<00:00,     589kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T67.1k/? [00:00<00:00,     559kB/s]\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE265k/? [00:00<00:00,     812kB/s]\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE128k/? [00:00<00:00,     540kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T32.0k/? [00:00<00:00,     206kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T91.3k/? [00:00<00:00,     360kB/s]\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE470k/? [00:00<00:00,    1.26MB/s]\u001b[A\n",
            "\n",
            "Transferring:  50% 3/6 [00:02<00:02,  1.32file/s{'info': ''}]\n",
            "\n",
            "                                                                                \u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE708k/? [00:00<00:00,    1.63MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T60.5k/? [00:00<00:00,     162kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE271k/? [00:00<00:00,     629kB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T76.7k/? [00:00<00:00,     163kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/TE926k/? [00:00<00:00,    1.80MB/s]\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE419k/? [00:00<00:00,     866kB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.18M/? [00:00<00:00,    2.10MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE117k/? [00:00<00:00,     232kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE592k/? [00:00<00:00,    1.10MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.65M/? [00:00<00:00,    2.95MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE153k/? [00:00<00:00,     270kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE825k/? [00:00<00:00,    1.45MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T2.31M/? [00:00<00:00,    4.12MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE205k/? [00:00<00:00,     345kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.13M/? [00:00<00:00,    2.00MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T3.11M/? [00:01<00:00,    5.37MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE285k/? [00:00<00:00,     478kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.55M/? [00:00<00:00,    2.69MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T4.43M/? [00:01<00:00,    7.88MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE392k/? [00:00<00:00,     659kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T2.09M/? [00:01<00:00,    3.54MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T6.24M/? [00:01<00:00,    11.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE520k/? [00:01<00:00,     849kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T2.85M/? [00:01<00:00,    4.80MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T8.33M/? [00:01<00:00,    14.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE704k/? [00:01<00:00,    1.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T3.91M/? [00:01<00:00,    6.64MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T11.4M/? [00:01<00:00,    19.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/TE964k/? [00:01<00:00,    1.60MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T5.27M/? [00:01<00:00,    8.87MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T15.6M/? [00:01<00:00,    26.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.28M/? [00:01<00:00,    2.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T7.19M/? [00:01<00:00,    12.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T21.3M/? [00:01<00:00,    36.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T1.74M/? [00:01<00:00,    2.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T9.88M/? [00:01<00:00,    16.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T28.8M/? [00:01<00:00,    48.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T2.36M/? [00:01<00:00,    3.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Transferring:  67% 4/6 [00:04<00:01,  1.03file/s{'info': ''}]\n",
            "\n",
            "\n",
            "                                                                                \u001b[A\u001b[A\u001b[A\n",
            "  0%|          |https://dagshub.com/Abuynits/T35.9M/? [00:01<00:00,    56.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T3.25M/? [00:01<00:00,    5.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Transferring:  83% 5/6 [00:04<00:00,  1.45file/s{'info': ''}]\n",
            "                                                                                \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T4.44M/? [00:01<00:00,    7.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "  0%|          |https://dagshub.com/Abuynits/T6.03M/? [00:01<00:00,    10.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Transferring: 100% 6/6 [00:04<00:00,  1.86file/s{'info': ''}]\n",
            "\n",
            "\n",
            "\n",
            "Checkout:   0% 0/1 [00:00<?, ?file/s{'info': ''}] \n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/66/10.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/8.38M [00:00<?, ?B/s{'info': ''}]                                     \u001b[A\n",
            "Checkout: 100% 1/1 [00:00<00:00, 37.69file/s{'info': ''}]\n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/6f/b0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/43.9M [00:00<?, ?B/s{'info': ''}]                                     \u001b[A\n",
            "  0% Checkout|          |11/? [00:00<00:00,  4.44file/s]\n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/e6/c0.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/13.0M [00:00<?, ?B/s{'info': ''}]                                     \u001b[A\n",
            "  0% Checkout|          |12/? [00:00<00:00,  4.44file/s]\n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/c7/70.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/155k [00:00<?, ?B/s{'info': ''}]                                      \u001b[A\n",
            "  0% Checkout|          |13/? [00:00<00:00,  4.44file/s]\n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/c3/70.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/119k [00:00<?, ?B/s{'info': ''}]                                      \u001b[A\n",
            "  0% Checkout|          |14/? [00:00<00:00,  4.44file/s]\n",
            "!\u001b[A\n",
            "  0%|          |/content/TE_Connect/.dvc/cache/50/80.00/? [00:00<?,        ?B/s]\u001b[A\n",
            "  0% 0.00/13.0M [00:00<?, ?B/s{'info': ''}]                                     \u001b[A\n",
            "\u001b[32mA\u001b[0m       data/model/model.pkl\n",
            "\u001b[32mA\u001b[0m       data/te_ai_cup_sales_forecasting_data.csv\n",
            "\u001b[32mA\u001b[0m       data/regular_data.pkl\n",
            "\u001b[32mA\u001b[0m       data/transformations/input_transformations.pkl\n",
            "\u001b[32mA\u001b[0m       data/transformations/output_transformations.pkl\n",
            "\u001b[32mA\u001b[0m       data/transformed_data.pkl\n",
            "6 files added and 9 files failed\n",
            "\u001b[31mERROR\u001b[39m: failed to pull data from the cloud - Checkout failed for following targets:\n",
            "/content/TE_Connect/data/dicts/test_dict.pkl\n",
            "/content/TE_Connect/data/dicts/train_dict.pkl\n",
            "/content/TE_Connect/data/dicts/validation_dict.pkl\n",
            "/content/TE_Connect/data/model_io/test_x.pkl\n",
            "/content/TE_Connect/data/model_io/test_y.pkl\n",
            "/content/TE_Connect/data/model_io/train_x.pkl\n",
            "/content/TE_Connect/data/model_io/train_y.pkl\n",
            "/content/TE_Connect/data/model_io/validation_x.pkl\n",
            "/content/TE_Connect/data/model_io/validation_y.pkl\n",
            "Is your cache up to date?\n",
            "<\u001b[36mhttps://error.dvc.org/missing-files\u001b[39m>\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING\n",
        "\n",
        "run the following cell after alterning any of the constant files. Track the progress live in the MLflow UI."
      ],
      "metadata": {
        "id": "u0csTn3Kwbfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pandas==1.4.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uITDuso95EmJ",
        "outputId": "b7660e01-581e-40dc-9972-92c24852dcd5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.4.1\n",
            "  Downloading pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.1) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.1) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.1) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.4.1) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 src/data_prep.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB2uvZCzy_X_",
        "outputId": "5b0346de-eda4-421f-e6bd-b096bd250005"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"src/data_prep.py\", line 1, in <module>\n",
            "    from visualization import *\n",
            "  File \"/content/TE_Connect/src/visualization.py\", line 1, in <module>\n",
            "    from model_constants import *\n",
            "  File \"/content/TE_Connect/src/model_constants.py\", line 1, in <module>\n",
            "    from data_constants import *\n",
            "  File \"/content/TE_Connect/src/data_constants.py\", line 1, in <module>\n",
            "    from filepaths_constants import *\n",
            "  File \"/content/TE_Connect/src/filepaths_constants.py\", line 7, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/__init__.py\", line 218, in <module>\n",
            "    from torch._C import *  # noqa: F403\n",
            "RuntimeError: KeyboardInterrupt: <EMPTY MESSAGE>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dvc repro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJQvimqfymK6",
        "outputId": "baf20c58-872d-4ec4-fe43-d46f129d2472"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r!\rIf DVC froze, see `hardlink_lock` in <\u001b[36mhttps://man.dvc.org/config#core\u001b[39m>\r                                                                      \r'data/te_ai_cup_sales_forecasting_data.csv.dvc' didn't change, skipping\n",
            "Stage 'data_prep' didn't change, skipping\n",
            "Running stage 'train_model':\n",
            "> python3 src/train.py\n",
            "reading data from files..\n",
            "creating datasets...\n",
            "batches in train dl: 58\n",
            "batches in valid dl: 17\n",
            "batches in test dl: 6\n",
            " Input seq:\n",
            " tensor([[0.0575, 0.0418, 0.5470],\n",
            "        [0.2281, 0.2498, 0.2887],\n",
            "        [0.3546, 0.3473, 0.3743],\n",
            "        [0.3596, 0.3664, 0.3429],\n",
            "        [0.1309, 0.1490, 0.2641],\n",
            "        [0.2177, 0.1982, 0.4298],\n",
            "        [0.0442, 0.0828, 0.0405],\n",
            "        [0.4069, 0.4995, 0.2079],\n",
            "        [0.3057, 0.3958, 0.1751],\n",
            "        [0.5903, 0.7651, 0.1708]], device='cuda:0')\n",
            " target seq:\n",
            " tensor([[5.9034e-01, 7.6510e-01, 1.7078e-01],\n",
            "        [4.9964e-02, 5.5491e-02, 2.8533e-01],\n",
            "        [1.5955e-02, 6.2382e-04, 8.4930e-01],\n",
            "        [7.9892e-02, 8.7558e-02, 2.9076e-01],\n",
            "        [4.7648e-02, 2.6242e-02, 7.0154e-01]], device='cuda:0')\n",
            " Out seq:\n",
            " tensor([[0.0500],\n",
            "        [0.0160],\n",
            "        [0.0799],\n",
            "        [0.0476],\n",
            "        [0.0469]], device='cuda:0')\n",
            " Input seq:\n",
            " tensor([[0.1015, 0.0809, 0.0826],\n",
            "        [0.3001, 0.1224, 0.2284],\n",
            "        [0.0930, 0.0254, 0.3870],\n",
            "        [0.1849, 0.0967, 0.1620],\n",
            "        [0.5961, 0.2490, 0.2149],\n",
            "        [0.4011, 0.1197, 0.3458],\n",
            "        [0.5928, 0.1583, 0.3971],\n",
            "        [0.4162, 0.1102, 0.4010],\n",
            "        [0.0490, 0.0278, 0.1849],\n",
            "        [0.2460, 0.1826, 0.0772]], device='cuda:0')\n",
            " target seq:\n",
            " tensor([[0.2460, 0.1826, 0.0772],\n",
            "        [0.1914, 0.0831, 0.2145],\n",
            "        [0.2320, 0.1143, 0.1743],\n",
            "        [0.3317, 0.1708, 0.1573],\n",
            "        [0.1270, 0.0301, 0.4465]], device='cuda:0')\n",
            " Out seq:\n",
            " tensor([[0.1914],\n",
            "        [0.2320],\n",
            "        [0.3317],\n",
            "        [0.1270],\n",
            "        [0.2269]], device='cuda:0')\n",
            "torch.Size([2048, 10, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "torch.Size([2048, 5, 1])\n",
            "torch.Size([2048, 10, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "time_transformer(\n",
            "  (enc): time_encoder(\n",
            "    (enc_inp_layer): Linear(in_features=3, out_features=512, bias=True)\n",
            "    (pos_enc): Pos_Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (enc_block): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (enc): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dec): time_decoder(\n",
            "    (dec_inp_layer): Linear(in_features=3, out_features=512, bias=True)\n",
            "    (dec_block): TransformerDecoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (multihead_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (dropout3): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (dec): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (linear_output_mapping): Linear(in_features=512, out_features=1, bias=True)\n",
            "    (linear_input_mapping): Linear(in_features=512, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "total trainable parameters: 36788228\n",
            "mlflow not authenticated, running through git\n",
            "epoch 0: avg train loss: 2.0587398711576617 avg val loss: 0.041078295558691025\n",
            "\ttrain accuracy: 0.41916602849960327\tbias: 0.09417860954999924\n",
            "\tvalid accuracy: 0.4513256549835205\tbias: -0.08336642384529114\n",
            "epoch 1: avg train loss: 0.03614200941721531 avg val loss: 0.045038722455501556\n",
            "\ttrain accuracy: 0.43328773975372314\tbias: 0.03947761654853821\n",
            "\tvalid accuracy: 0.45600390434265137\tbias: -0.16232603788375854\n",
            "epoch 2: avg train loss: 0.03562240629357691 avg val loss: 0.042576100677251816\n",
            "\ttrain accuracy: 0.4442533850669861\tbias: 0.05589410662651062\n",
            "\tvalid accuracy: 0.4614110589027405\tbias: -0.12944576144218445\n",
            "epoch 3: avg train loss: 0.03531932099458756 avg val loss: 0.04302067682147026\n",
            "\ttrain accuracy: 0.4362437129020691\tbias: 0.03419414907693863\n",
            "\tvalid accuracy: 0.4651392102241516\tbias: -0.14913664758205414\n",
            "epoch 4: avg train loss: 0.03496900000879367 avg val loss: 0.04359673336148262\n",
            "\ttrain accuracy: 0.4534727931022644\tbias: 0.02124476060271263\n",
            "\tvalid accuracy: 0.45546579360961914\tbias: -0.1491987109184265\n",
            "epoch 5: avg train loss: 0.03460993249199952 avg val loss: 0.04397397115826607\n",
            "\ttrain accuracy: 0.44402819871902466\tbias: 0.026736963540315628\n",
            "\tvalid accuracy: 0.45065420866012573\tbias: -0.14201602339744568\n",
            "epoch 6: avg train loss: 0.03424548332696305 avg val loss: 0.044794630259275436\n",
            "\ttrain accuracy: 0.44493454694747925\tbias: 0.012004763819277287\n",
            "\tvalid accuracy: 0.46500563621520996\tbias: -0.18483871221542358\n",
            "epoch 7: avg train loss: 0.03387870790851412 avg val loss: 0.04234123229980469\n",
            "\ttrain accuracy: 0.4226011037826538\tbias: 0.07355566322803497\n",
            "\tvalid accuracy: 0.4535669684410095\tbias: -0.11363635212182999\n",
            "epoch 8: avg train loss: 0.03349586943528553 avg val loss: 0.047847930341959\n",
            "\ttrain accuracy: 0.44253045320510864\tbias: -0.018624886870384216\n",
            "\tvalid accuracy: 0.44777166843414307\tbias: -0.19197358191013336\n",
            "epoch 9: avg train loss: 0.03307313149649949 avg val loss: 0.045123908668756485\n",
            "\ttrain accuracy: 0.45850884914398193\tbias: 0.027317050844430923\n",
            "\tvalid accuracy: 0.466267466545105\tbias: -0.19338536262512207\n",
            "epoch 10: avg train loss: 0.032330853035898235 avg val loss: 0.044610004872083664\n",
            "\ttrain accuracy: 0.4659993648529053\tbias: -0.019836846739053726\n",
            "\tvalid accuracy: 0.45679593086242676\tbias: -0.18623663485050201\n",
            "epoch 11: avg train loss: 0.03143621163047075 avg val loss: 0.04527894780039787\n",
            "\ttrain accuracy: 0.46061450242996216\tbias: 0.0036632209084928036\n",
            "\tvalid accuracy: 0.46121883392333984\tbias: -0.17360886931419373\n",
            "epoch 12: avg train loss: 0.030883976031283837 avg val loss: 0.04279179126024246\n",
            "\ttrain accuracy: 0.44130241870880127\tbias: 0.009532440453767776\n",
            "\tvalid accuracy: 0.4562423825263977\tbias: -0.15107987821102142\n",
            "epoch 13: avg train loss: 0.03071422314999183 avg val loss: 0.04213869199156761\n",
            "\ttrain accuracy: 0.4437517523765564\tbias: 0.05966856703162193\n",
            "\tvalid accuracy: 0.46392643451690674\tbias: -0.11096695065498352\n",
            "epoch 14: avg train loss: 0.03057635172770213 avg val loss: 0.044390659779310226\n",
            "\ttrain accuracy: 0.4407636523246765\tbias: 0.019967926666140556\n",
            "\tvalid accuracy: 0.4589904546737671\tbias: -0.17100414633750916\n",
            "epoch 15: avg train loss: 0.030479184521478138 avg val loss: 0.042712971568107605\n",
            "\ttrain accuracy: 0.4335797429084778\tbias: 0.03789258748292923\n",
            "\tvalid accuracy: 0.459634006023407\tbias: -0.14816313982009888\n",
            "epoch 16: avg train loss: 0.03042276836295207 avg val loss: 0.045461446046829224\n",
            "\ttrain accuracy: 0.45512914657592773\tbias: 0.008224579505622387\n",
            "\tvalid accuracy: 0.4482489824295044\tbias: -0.15940149128437042\n",
            "Traceback (most recent call last):\n",
            "  File \"src/train.py\", line 148, in <module>\n",
            "    avg_train_loss = train_epoch(train_dl, e)\n",
            "  File \"src/train.py\", line 99, in train_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 488, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[31mERROR\u001b[39m: failed to reproduce 'train_model': failed to run: python3 src/train.py, exited with -2\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!MLFLOW_TRACKING_URI=https://dagshub.com/Abuynits/TE_Connect.mlflow \\\n",
        "MLFLOW_TRACKING_USERNAME={USER_NAME} \\\n",
        "MLFLOW_TRACKING_PASSWORD={TOKEN}  \\\n",
        "dvc repro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96jPswGyoP93",
        "outputId": "31b232a2-6d5c-4149-a275-344291c25e6f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r!\rIf DVC froze, see `hardlink_lock` in <\u001b[36mhttps://man.dvc.org/config#core\u001b[39m>\r                                                                      \r'data/te_ai_cup_sales_forecasting_data.csv.dvc' didn't change, skipping\n",
            "Stage 'data_prep' didn't change, skipping\n",
            "Running stage 'train_model':\n",
            "> python3 src/train.py\n",
            "reading data from files..\n",
            "creating datasets...\n",
            "batches in train dl: 58\n",
            "batches in valid dl: 17\n",
            "batches in test dl: 6\n",
            " Input seq:\n",
            " tensor([[0.0912, 0.0529, 0.4103],\n",
            "        [0.2691, 0.2661, 0.1836],\n",
            "        [0.2525, 0.2508, 0.1836],\n",
            "        [0.2691, 0.2661, 0.1836],\n",
            "        [0.4191, 0.4038, 0.1836],\n",
            "        [0.1162, 0.1454, 0.1298],\n",
            "        [0.1363, 0.1655, 0.1298],\n",
            "        [0.1363, 0.1655, 0.1298],\n",
            "        [0.1463, 0.1755, 0.1298],\n",
            "        [0.1787, 0.2675, 0.0325]], device='cuda:0')\n",
            " target seq:\n",
            " tensor([[0.1787, 0.2675, 0.0325],\n",
            "        [0.1941, 0.2861, 0.0326],\n",
            "        [0.1787, 0.2675, 0.0325],\n",
            "        [0.1787, 0.2675, 0.0325],\n",
            "        [0.2249, 0.3233, 0.0326]], device='cuda:0')\n",
            " Out seq:\n",
            " tensor([[0.1941],\n",
            "        [0.1787],\n",
            "        [0.1787],\n",
            "        [0.2249],\n",
            "        [0.2273]], device='cuda:0')\n",
            " Input seq:\n",
            " tensor([[0.1278, 0.5216, 0.0656],\n",
            "        [0.1024, 0.4139, 0.0876],\n",
            "        [0.1143, 0.4959, 0.0559],\n",
            "        [0.1132, 0.4900, 0.0555],\n",
            "        [0.1168, 0.4159, 0.6605],\n",
            "        [0.1074, 0.4633, 0.0514],\n",
            "        [0.1200, 0.4996, 0.0624],\n",
            "        [0.1156, 0.5087, 0.0552],\n",
            "        [0.1538, 0.4345, 0.3030],\n",
            "        [0.2110, 0.5590, 0.1205]], device='cuda:0')\n",
            " target seq:\n",
            " tensor([[0.2110, 0.5590, 0.1205],\n",
            "        [0.1083, 0.4470, 0.0597],\n",
            "        [0.1093, 0.4393, 0.0696],\n",
            "        [0.1267, 0.6213, 0.0528],\n",
            "        [0.1074, 0.4324, 0.0697]], device='cuda:0')\n",
            " Out seq:\n",
            " tensor([[0.1083],\n",
            "        [0.1093],\n",
            "        [0.1267],\n",
            "        [0.1074],\n",
            "        [0.1156]], device='cuda:0')\n",
            "torch.Size([2048, 10, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "torch.Size([2048, 5, 1])\n",
            "torch.Size([2048, 10, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "torch.Size([2048, 5, 3])\n",
            "time_transformer(\n",
            "  (enc): time_encoder(\n",
            "    (enc_inp_layer): Linear(in_features=3, out_features=512, bias=True)\n",
            "    (pos_enc): Pos_Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (enc_block): TransformerEncoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.0, inplace=False)\n",
            "      (dropout2): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (enc): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dec): time_decoder(\n",
            "    (dec_inp_layer): Linear(in_features=3, out_features=512, bias=True)\n",
            "    (dec_block): TransformerDecoderLayer(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (multihead_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout1): Dropout(p=0.0, inplace=False)\n",
            "      (dropout2): Dropout(p=0.0, inplace=False)\n",
            "      (dropout3): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (dec): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "          (dropout3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "          (dropout3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "          (dropout3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.0, inplace=False)\n",
            "          (dropout2): Dropout(p=0.0, inplace=False)\n",
            "          (dropout3): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (linear_output_mapping): Linear(in_features=512, out_features=1, bias=True)\n",
            "    (linear_input_mapping): Linear(in_features=512, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "total trainable parameters: 36788228\n",
            "epoch 0: avg train loss: 0.6812393700608947 avg val loss: 0.037756044417619705\n",
            "\ttrain accuracy: 0.4063628315925598\ttrain bias: 0.23164165019989014\n",
            "\tvalid accuracy: 0.48155421018600464\tvalid bias: 0.014680505730211735\n",
            "epoch 1: avg train loss: 0.024827740075756628 avg val loss: 0.037169989198446274\n",
            "\ttrain accuracy: 0.5279299020767212\ttrain bias: -0.06501041352748871\n",
            "\tvalid accuracy: 0.5236719846725464\tvalid bias: -0.17952276766300201\n",
            "epoch 2: avg train loss: 0.016587595258439745 avg val loss: 0.02886396087706089\n",
            "\ttrain accuracy: 0.519141674041748\ttrain bias: 0.1298990696668625\n",
            "\tvalid accuracy: 0.5581406354904175\tvalid bias: -0.009963640943169594\n",
            "epoch 3: avg train loss: 0.014483971568076483 avg val loss: 0.03612970933318138\n",
            "\ttrain accuracy: 0.3618379831314087\ttrain bias: 0.4607687294483185\n",
            "\tvalid accuracy: 0.4944370985031128\tvalid bias: 0.26869550347328186\n",
            "epoch 4: avg train loss: 0.014174424152765387 avg val loss: 0.04028986394405365\n",
            "\ttrain accuracy: 0.2972227931022644\ttrain bias: 0.5445700287818909\n",
            "\tvalid accuracy: 0.4487748146057129\tvalid bias: 0.3446263074874878\n",
            "epoch 5: avg train loss: 0.014075052300444948 avg val loss: 0.03880804404616356\n",
            "\ttrain accuracy: 0.3233180642127991\ttrain bias: 0.5147193074226379\n",
            "\tvalid accuracy: 0.45762568712234497\tvalid bias: 0.32672685384750366\n",
            "epoch 6: avg train loss: 0.013991787066459828 avg val loss: 0.037802714854478836\n",
            "\ttrain accuracy: 0.31065833568573\ttrain bias: 0.5322838425636292\n",
            "\tvalid accuracy: 0.4680227041244507\tvalid bias: 0.3160816729068756\n",
            "epoch 7: avg train loss: 0.013903982685177393 avg val loss: 0.03625301644206047\n",
            "\ttrain accuracy: 0.32044386863708496\ttrain bias: 0.5193085074424744\n",
            "\tvalid accuracy: 0.48058193922042847\tvalid bias: 0.31414899230003357\n",
            "epoch 8: avg train loss: 0.01381916592835808 avg val loss: 0.035470832139253616\n",
            "\ttrain accuracy: 0.3502737283706665\ttrain bias: 0.4997217357158661\n",
            "\tvalid accuracy: 0.48429256677627563\tvalid bias: 0.32283222675323486\n",
            "epoch 9: avg train loss: 0.01375851786423863 avg val loss: 0.03459405526518822\n",
            "\ttrain accuracy: 0.39671647548675537\ttrain bias: 0.43726783990859985\n",
            "\tvalid accuracy: 0.4931531548500061\tvalid bias: 0.28573763370513916\n",
            "epoch 10: avg train loss: 0.013685492695825616 avg val loss: 0.03186545521020889\n",
            "\ttrain accuracy: 0.40507858991622925\ttrain bias: 0.4151778221130371\n",
            "\tvalid accuracy: 0.5197812914848328\tvalid bias: 0.25480079650878906\n",
            "epoch 11: avg train loss: 0.013632491347645454 avg val loss: 0.032576095312833786\n",
            "\ttrain accuracy: 0.3982585668563843\ttrain bias: 0.4376530349254608\n",
            "\tvalid accuracy: 0.5092859268188477\tvalid bias: 0.27823853492736816\n",
            "epoch 12: avg train loss: 0.013583509186906257 avg val loss: 0.03125999495387077\n",
            "\ttrain accuracy: 0.4349833130836487\ttrain bias: 0.3921990394592285\n",
            "\tvalid accuracy: 0.5191754102706909\tvalid bias: 0.2517935633659363\n",
            "epoch 13: avg train loss: 0.013599170080045443 avg val loss: 0.03070169687271118\n",
            "\ttrain accuracy: 0.43399524688720703\ttrain bias: 0.38547638058662415\n",
            "\tvalid accuracy: 0.5245697498321533\tvalid bias: 0.2415543496608734\n",
            "epoch 14: avg train loss: 0.013526516788695266 avg val loss: 0.029065871611237526\n",
            "\ttrain accuracy: 0.44512224197387695\ttrain bias: 0.3665962517261505\n",
            "\tvalid accuracy: 0.5473485589027405\tvalid bias: 0.21449901163578033\n",
            "epoch 15: avg train loss: 0.013505246777266607 avg val loss: 0.02840542234480381\n",
            "\ttrain accuracy: 0.4567122459411621\ttrain bias: 0.3486595153808594\n",
            "\tvalid accuracy: 0.5442676544189453\tvalid bias: 0.2349226474761963\n",
            "epoch 16: avg train loss: 0.01350538592068205 avg val loss: 0.0285689365118742\n",
            "\ttrain accuracy: 0.45302248001098633\ttrain bias: 0.3728616535663605\n",
            "\tvalid accuracy: 0.5376846194267273\tvalid bias: 0.23699167370796204\n",
            "epoch 17: avg train loss: 0.013482088671564764 avg val loss: 0.025806104764342308\n",
            "\ttrain accuracy: 0.4959278702735901\ttrain bias: 0.30822545289993286\n",
            "\tvalid accuracy: 0.5646271705627441\tvalid bias: 0.1874540150165558\n",
            "epoch 18: avg train loss: 0.013456191791165775 avg val loss: 0.025962114334106445\n",
            "\ttrain accuracy: 0.5015338659286499\ttrain bias: 0.2865661084651947\n",
            "\tvalid accuracy: 0.5609326362609863\tvalid bias: 0.19290660321712494\n",
            "epoch 19: avg train loss: 0.013455439952084954 avg val loss: 0.02721301093697548\n",
            "\ttrain accuracy: 0.4727281332015991\ttrain bias: 0.33699196577072144\n",
            "\tvalid accuracy: 0.5594618320465088\tvalid bias: 0.19114620983600616\n",
            "epoch 20: avg train loss: 0.01344238552513541 avg val loss: 0.02541274204850197\n",
            "\ttrain accuracy: 0.5081499218940735\ttrain bias: 0.2825395464897156\n",
            "\tvalid accuracy: 0.5733098983764648\tvalid bias: 0.17188753187656403\n",
            "Traceback (most recent call last):\n",
            "  File \"src/train.py\", line 148, in <module>\n",
            "    avg_train_loss = train_epoch(train_dl, e)\n",
            "  File \"src/train.py\", line 94, in train_epoch\n",
            "    epoch_train_loss += loss.item() * x.size(0)\n",
            "KeyboardInterrupt\n",
            "\u001b[31mERROR\u001b[39m: failed to reproduce 'train_model': failed to run: python3 src/train.py, exited with -2\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eeB_qLjswc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}